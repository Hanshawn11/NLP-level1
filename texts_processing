# 处理文本,清洗数据包

import pandas as pd
import numpy as np
import gensim.downloader as api
vector_model = api.load("glove-wiki-gigaword-100")  # Since this is a practice, I use this simple word2vec model
                                                    # which just has 100_dim to represent a word. 
import nltk
import re
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
for w in ['!',',','.','?','-s','-ly','</s>','s',"'s",'...','More','0','1','2',
         '3','4','5','6','7','8','9','I']:
    stop_words.append(w)

df = pd.read_csv("/content/score_reviews.csv")
df.loc[df.score<3,'score'] = 0
df.loc[df.score>=3,'score'] = 1

def getWordVecs(wordList,model):
    vecs = []
    for word in wordList:
        try:
            vecs.append(model[word])
        except KeyError:
            continue
    return np.array(vecs, dtype='float')

#将情感tag 1/0 以及句子的向量表达 写入新的csv文件

new_fileheader = ['senti','txt_vec']
new_csvfile = open('senti_txtvec.csv','w')
new_writer = csv.writer(new_csvfile)
new_writer.writerow(new_fileheader)

i = 0
for i in range(0, len(df)):
    txt = (df.iloc[i]['reviews']).lower()
    score=(df.iloc[i]['score'])
    txt = nltk.word_tokenize(txt)
    txt = [word for word in txt if word not in stop_words]
    txt = [word for word in txt if word in vector_model.vocab]
    vec = getWordVecs(txt,vector_model)
    mean_vec = sum(np.array(vec)/len(vec)) # for each sentence, the mean vector of every words 
                                           #in it is used to represent this sentence. 
    new_writer.writerow([score,mean_vec])
    #print(mean_vec)
    i += 1
    
new_csvfile.close()
